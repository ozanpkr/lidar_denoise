{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H3nhqCa8NIhX"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import os\n",
    "from extern.atmos_models import LISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h4q6EszpOfKX"
   },
   "outputs": [],
   "source": [
    "def get_files_by_extension(directory, allowed_extensions):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in allowed_extensions):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "l3lZAKDJiBuO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "org_dataset_path = \"data/a2d2\"\n",
    "noisy_dataset_path = \"data/noisy_dataset/\"\n",
    "clean_dataset_path = \"data/clean_dataset/\"\n",
    "allowed_file_types = [\".npz\"]\n",
    "org_point_clouds_paths = get_files_by_extension(org_dataset_path, allowed_file_types)\n",
    "print(len(org_point_clouds_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GBPSOpUCYdC2"
   },
   "outputs": [],
   "source": [
    "lisa = LISA(atm_model='rain',rmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "udPIGid1NIaM"
   },
   "outputs": [],
   "source": [
    "for org_point_cloud_path in org_point_clouds_paths:    \n",
    "      file_name= os.path.basename(org_point_cloud_path)\n",
    "        \n",
    "      noisy_file_name= file_name[:-4]+\"_noisy.npz\"\n",
    "      clean_file_name= file_name[:-4]+\"_clean.npz\"\n",
    "        \n",
    "      noisy_file_path = join(\"data/noisy_dataset\", noisy_file_name)\n",
    "      clean_file_path = join(\"data/clean_dataset\", clean_file_name)\n",
    "\n",
    "      org_lidar_data = np.load(org_point_cloud_path)\n",
    "        \n",
    "      org_lidar_points= org_lidar_data['pcloud_points']\n",
    "        \n",
    "      org_lidar_reflectance = org_lidar_data['pcloud_attr.reflectance']\n",
    "      normalized_lidar_reflectance= (org_lidar_reflectance - org_lidar_reflectance.min()) / (org_lidar_reflectance.max() - org_lidar_reflectance.min())\n",
    "        \n",
    "      org_data = np.column_stack((org_lidar_points, normalized_lidar_reflectance))\n",
    "\n",
    "      augmented_point_cloud = lisa.augment(org_data,50) # rain rate 50 mm/hr\n",
    "\n",
    "      lost_points_indices = np.where(augmented_point_cloud[:, 4] == 0)[0]\n",
    "      noisy_point_cloud_ = np.delete(org_lidar_points, lost_points_indices, axis=0)\n",
    "    \n",
    "      np.savez(noisy_file_path, lidar=noisy_point_cloud_)\n",
    "      np.savez(clean_file_path, lidar=org_lidar_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXlvL2NjNIVf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
